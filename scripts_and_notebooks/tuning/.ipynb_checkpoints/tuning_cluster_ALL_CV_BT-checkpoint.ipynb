{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a73d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Wed Aug 20 17:30 2022\n",
    "\n",
    "Script to tune simple parameterisations in ALL (best-estimate), CV (cross-validation) and BT (bootstrap)\n",
    "\n",
    "@author: Clara Burgard\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c367c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import multimelt.useful_functions as uf\n",
    "import multimelt.melt_functions as meltf\n",
    "from multimelt.constants import *\n",
    "from scipy import stats\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55286d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c81a82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac658f2",
   "metadata": {},
   "source": [
    "READ IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5789731",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path='/bettik/burgardc/'\n",
    "outputpath_simple_all = home_path+'DATA/BASAL_MELT_PARAM/interim/SIMPLE/nemo_5km_06161821_oneFRIS/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da1830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5ad718-fb71-40c3-9a75-6707d494ca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nisf_list_orig = [10, 11, 12, 13, 18, 22, 23, 24, 25, 30, 31, 33, 38, 39, 40, 42, 43, 44, 45, 47, 48, \n",
    "                  51, 52, 53, 54, 55, 58, 61, 65, 66, 69, 70, 71, 73, 75]\n",
    "run_list = ['OPM006','OPM016','OPM018','OPM021']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8dc797-22bf-41bc-80e3-35a25821f40e",
   "metadata": {},
   "source": [
    "ALL (tune over the whole sample) => best-estimate parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7a216a-1c54-4426-8b2c-f40a575d4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE DATA\n",
    "\n",
    "file_isf_list = []\n",
    "thermal_forcing_term_list = []\n",
    "target_melt_list = []\n",
    "isf_area_lit = []\n",
    "GL_flux_list = []\n",
    "weights_list = []\n",
    "\n",
    "for n,nemo_run in enumerate(run_list):\n",
    "\n",
    "    # File mask\n",
    "    inputpath_mask = home_path+'DATA/BASAL_MELT_PARAM/interim/ANTARCTICA_IS_MASKS/nemo_5km_'+nemo_run+'/'\n",
    "    file_isf_orig = xr.open_dataset(inputpath_mask+'nemo_5km_isf_masks_and_info_and_distance_new_oneFRIS.nc')\n",
    "    nonnan_Nisf = file_isf_orig['Nisf'].where(np.isfinite(file_isf_orig['front_bot_depth_max']), drop=True).astype(int)\n",
    "    file_isf_nonnan = file_isf_orig.sel(Nisf=nonnan_Nisf)\n",
    "    large_isf = file_isf_nonnan['Nisf'].where(file_isf_nonnan['isf_area_here'] >= 2500, drop=True)\n",
    "    file_isf = file_isf_nonnan.sel(Nisf=large_isf)\n",
    "    file_isf_list.append(file_isf)\n",
    "\n",
    "    # File for param\n",
    "    outputpath_simple = home_path+'DATA/BASAL_MELT_PARAM/interim/SIMPLE/nemo_5km_'+nemo_run+'/'\n",
    "    if bottom:\n",
    "        thermal_forcing_2D = xr.open_dataset(outputpath_simple+'thermal_forcing_2D_bottom_for_tuning_corrected_oneFRIS.nc')\n",
    "        thermal_forcing_file = xr.open_dataset(outputpath_simple+'thermal_forcing_term_bottom_for_linreg_corrected_oneFRIS.nc')\n",
    "    else:\n",
    "        thermal_forcing_2D = xr.open_dataset(outputpath_simple+'thermal_forcing_2D_for_tuning_corrected_oneFRIS.nc')\n",
    "        thermal_forcing_file = xr.open_dataset(outputpath_simple+'thermal_forcing_term_for_linreg_corrected_oneFRIS.nc')\n",
    "\n",
    "    thermal_forcing_term = thermal_forcing_file['thermal_forcing_term'].sel(Nisf=file_isf.Nisf)\n",
    "    thermal_forcing_term = thermal_forcing_term.assign_coords({'time': np.arange(1,len(thermal_forcing_term.time)+1)+n*50})\n",
    "    thermal_forcing_term_list.append(thermal_forcing_term)\n",
    "\n",
    "    # File for target\n",
    "    outputpath_melt = home_path+'DATA/BASAL_MELT_PARAM/processed/MELT_RATE/nemo_5km_'+nemo_run+'/'\n",
    "    NEMO_melt_rates_1D = xr.open_dataset(outputpath_melt+'melt_rates_1D_NEMO_oneFRIS.nc')\n",
    "    target_melt_Gt_yr = NEMO_melt_rates_1D['melt_Gt_per_y_tot'].sel(Nisf=file_isf.Nisf).sel(time=thermal_forcing_file.time)\n",
    "    target_melt_Gt_yr = target_melt_Gt_yr.assign_coords({'time': np.arange(1,len(target_melt_Gt_yr.time)+1)+n*50})\n",
    "    target_melt_list.append(target_melt_Gt_yr)\n",
    "    \n",
    "    # Mask of ice shelves\n",
    "    isf_mask = file_isf['ISF_mask'].where(file_isf['ISF_mask'] == file_isf.Nisf).sum('Nisf')\n",
    "    isf_mask = isf_mask.where(isf_mask)\n",
    "\n",
    "file_isf_all = xr.concat(file_isf_list, dim='nemo_run')\n",
    "#file_isf_all = file_isf_all.assign_coords({'nemo_run': run_list})\n",
    "\n",
    "thermal_forcing_all = xr.concat(thermal_forcing_term_list, dim='time')\n",
    "#thermal_forcing_all = thermal_forcing_all.assign_coords({'nemo_run': run_list})\n",
    "\n",
    "target_melt_all = xr.concat(target_melt_list, dim='time')\n",
    "#target_melt_all = target_melt_all.assign_coords({'nemo_run': run_list})\n",
    "\n",
    "# CONSTANTS AND TARGETS\n",
    "\n",
    "xx = file_isf_all.x\n",
    "yy = file_isf_all.y\n",
    "dx = (xx[2] - xx[1]).values\n",
    "dy = (yy[2] - yy[1]).values\n",
    "grid_cell_area = abs(dx*dy)\n",
    "\n",
    "# We need the big constant (which will be multiplied by thermal_forcing_factor and gamma to reach the integrated melt)\n",
    "big_constant_linear = rho_i * 10**-12 * yearinsec * grid_cell_area * melt_factor\n",
    "big_constant_quad = (c_po / L_i) * beta_coeff_lazero * (g/(2*f_coriolis))\n",
    "\n",
    "# the thermal forcing term just missing the gamma (big_constant_quad is added later for the quadratic params)\n",
    "thermal_forcing_plus = thermal_forcing_all * big_constant_linear \n",
    "\n",
    "target_melt_Gt_yr = target_melt_all \n",
    "\n",
    "nisf_list = nisf_list_orig\n",
    "\n",
    "# LINEAR REGRESSION\n",
    "# prepare data\n",
    "thermal_forcing_term_stacked = thermal_forcing_plus.sel(Nisf=nisf_list).stack(reg_coord=('Nisf', 'time'))  \n",
    "target_melt_stacked = target_melt_Gt_yr.sel(Nisf=nisf_list).stack(reg_coord=('Nisf', 'time'))  \n",
    "\n",
    "# choose domain and param\n",
    "metrics_all = None\n",
    "\n",
    "for dom in thermal_forcing_term_stacked.profile_domain:\n",
    "    metrics_list = [ ]\n",
    "\n",
    "    for mparam in thermal_forcing_term_stacked.param.values:\n",
    "\n",
    "        if bottom:\n",
    "            mparam0 = mparam[:-7:]\n",
    "        else:\n",
    "            mparam0 = mparam\n",
    "\n",
    "        if mparam0 != 'linear_local': # multiply by yet another constant\n",
    "            thermal_forcing_to_reg_stacked = thermal_forcing_term_stacked.sel(profile_domain=dom,param=mparam) * big_constant_quad\n",
    "        else:\n",
    "            thermal_forcing_to_reg_stacked = thermal_forcing_term_stacked.sel(profile_domain=dom,param=mparam)\n",
    "\n",
    "        if mparam0 != 'linear_local': # multiply by yet another constant\n",
    "            thermal_forcing_to_plot = thermal_forcing_plus.sel(Nisf=nisf_list).sel(profile_domain=dom,param=mparam) * big_constant_quad\n",
    "        else:\n",
    "            thermal_forcing_to_plot = thermal_forcing_plus.sel(Nisf=nisf_list).sel(profile_domain=dom,param=mparam)\n",
    "\n",
    "        # conduct the regression and compute the metrics\n",
    "        # we choose lstsq to force the intercept to be 0\n",
    "        res, resid, rnk, s = linalg.lstsq(thermal_forcing_to_reg_stacked.values[:, np.newaxis], \n",
    "                                          target_melt_stacked.values)\n",
    "\n",
    "\n",
    "\n",
    "        metrics_da = xr.DataArray(data=res.squeeze()).to_dataset(name='slope')\n",
    "\n",
    "        metrics_list.append(metrics_da)\n",
    "\n",
    "    metrics_dom = xr.concat(metrics_list, dim='param')\n",
    "    metrics_dom = metrics_dom.assign_coords({'param': thermal_forcing_term_stacked.param})\n",
    "\n",
    "    metrics_all = meltf.merge_over_dim(metrics_dom, metrics_all, 'profile_domain', dom.values)\n",
    "\n",
    "metrics_all.to_netcdf(outputpath_simple_all + 'gammas_simple_ALL.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aa6d81-4f9d-425b-98b4-43c91a532e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results for check\n",
    "dom = 1000\n",
    "for mparam in metrics_all.param:\n",
    "    print(str(mparam.values)+' '+str(metrics_all['slope'].sel(param=mparam).sel(profile_domain=dom).values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17af597b-14c2-45cb-a5f7-77cb16947a13",
   "metadata": {},
   "source": [
    "CROSS-VALIDATION OVER TIME BLOCKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f8d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_tt = None\n",
    "for tt in tqdm(range(1,14)):\n",
    "    \n",
    "    # DEFINE TIME BLOCK\n",
    "    \n",
    "    tblock_out = tt\n",
    "    if tblock_out == 0:\n",
    "        tblock_out, tblock_run, tblock_start, tblock_end = [False, False, False, False]\n",
    "\n",
    "    if tblock_out:\n",
    "        inputpath_chunk_info = '/bettik/burgardc/DATA/BASAL_MELT_PARAM/interim/T_S_PROF/'\n",
    "        info_file = pd.read_csv(inputpath_chunk_info+'info_chunks.txt',header=None, index_col=0)\n",
    "        tblock_run, tblock_start, tblock_end = info_file.loc[tblock_out]\n",
    "\n",
    "    # PREPARE DATA\n",
    "\n",
    "    file_isf_list = []\n",
    "    thermal_forcing_term_list = []\n",
    "    target_melt_list = []\n",
    "    isf_area_lit = []\n",
    "    GL_flux_list = []\n",
    "    weights_list = []\n",
    "\n",
    "    for n,nemo_run in enumerate(run_list):\n",
    "\n",
    "        # File mask\n",
    "        inputpath_mask = home_path+'DATA/BASAL_MELT_PARAM/interim/ANTARCTICA_IS_MASKS/nemo_5km_'+nemo_run+'/'\n",
    "        file_isf_orig = xr.open_dataset(inputpath_mask+'nemo_5km_isf_masks_and_info_and_distance_new_oneFRIS.nc')\n",
    "        nonnan_Nisf = file_isf_orig['Nisf'].where(np.isfinite(file_isf_orig['front_bot_depth_max']), drop=True).astype(int)\n",
    "        file_isf_nonnan = file_isf_orig.sel(Nisf=nonnan_Nisf)\n",
    "        large_isf = file_isf_nonnan['Nisf'].where(file_isf_nonnan['isf_area_here'] >= 2500, drop=True)\n",
    "        file_isf = file_isf_nonnan.sel(Nisf=large_isf)\n",
    "        file_isf_list.append(file_isf)\n",
    "\n",
    "        # File for param\n",
    "        outputpath_simple = home_path+'DATA/BASAL_MELT_PARAM/interim/SIMPLE/nemo_5km_'+nemo_run+'/'\n",
    "        if bottom:\n",
    "            thermal_forcing_2D = xr.open_dataset(outputpath_simple+'thermal_forcing_2D_bottom_for_tuning_corrected_oneFRIS.nc')\n",
    "            thermal_forcing_file = xr.open_dataset(outputpath_simple+'thermal_forcing_term_bottom_for_linreg_corrected_oneFRIS.nc')\n",
    "        else:\n",
    "            thermal_forcing_2D = xr.open_dataset(outputpath_simple+'thermal_forcing_2D_for_tuning_corrected_oneFRIS.nc')\n",
    "            thermal_forcing_file = xr.open_dataset(outputpath_simple+'thermal_forcing_term_for_linreg_corrected_oneFRIS.nc')\n",
    "        if nemo_run == tblock_run:\n",
    "            thermal_forcing_term = thermal_forcing_file['thermal_forcing_term'].sel(Nisf=file_isf.Nisf).drop_sel(time=range(tblock_start,tblock_end+1))\n",
    "        else:\n",
    "            thermal_forcing_term = thermal_forcing_file['thermal_forcing_term'].sel(Nisf=file_isf.Nisf)\n",
    "        thermal_forcing_term = thermal_forcing_term.assign_coords({'time': np.arange(1,len(thermal_forcing_term.time)+1)+n*50})\n",
    "        thermal_forcing_term_list.append(thermal_forcing_term)\n",
    "\n",
    "        # File for target\n",
    "        outputpath_melt = home_path+'DATA/BASAL_MELT_PARAM/processed/MELT_RATE/nemo_5km_'+nemo_run+'/'\n",
    "        NEMO_melt_rates_1D = xr.open_dataset(outputpath_melt+'melt_rates_1D_NEMO_oneFRIS.nc')\n",
    "        if nemo_run == tblock_run:\n",
    "            target_melt_Gt_yr = NEMO_melt_rates_1D['melt_Gt_per_y_tot'].sel(Nisf=file_isf.Nisf).sel(time=thermal_forcing_file.time).drop_sel(time=range(tblock_start,tblock_end+1))\n",
    "        else:\n",
    "            target_melt_Gt_yr = NEMO_melt_rates_1D['melt_Gt_per_y_tot'].sel(Nisf=file_isf.Nisf).sel(time=thermal_forcing_file.time)\n",
    "        target_melt_Gt_yr = target_melt_Gt_yr.assign_coords({'time': np.arange(1,len(target_melt_Gt_yr.time)+1)+n*50})\n",
    "        target_melt_list.append(target_melt_Gt_yr)\n",
    "\n",
    "        isf_mask = file_isf['ISF_mask'].where(file_isf['ISF_mask'] == file_isf.Nisf).sum('Nisf')\n",
    "        isf_mask = isf_mask.where(isf_mask)\n",
    "\n",
    "    file_isf_all = xr.concat(file_isf_list, dim='nemo_run')\n",
    "    #file_isf_all = file_isf_all.assign_coords({'nemo_run': run_list})\n",
    "\n",
    "    thermal_forcing_all = xr.concat(thermal_forcing_term_list, dim='time')\n",
    "    #thermal_forcing_all = thermal_forcing_all.assign_coords({'nemo_run': run_list})\n",
    "\n",
    "    target_melt_all = xr.concat(target_melt_list, dim='time')\n",
    "    #target_melt_all = target_melt_all.assign_coords({'nemo_run': run_list})\n",
    "    \n",
    "    # CONSTANTS AND TARGETS\n",
    "    \n",
    "    xx = file_isf_all.x\n",
    "    yy = file_isf_all.y\n",
    "    dx = (xx[2] - xx[1]).values\n",
    "    dy = (yy[2] - yy[1]).values\n",
    "    grid_cell_area = abs(dx*dy)\n",
    "    \n",
    "    # We need the big constant\n",
    "    big_constant_linear = rho_i * 10**-12 * yearinsec * grid_cell_area * melt_factor\n",
    "    big_constant_quad = (c_po / L_i) * beta_coeff_lazero * (g/(2*f_coriolis))\n",
    "    \n",
    "    # the thermal forcing term just missing the gamma (big_constant_quad is added later)\n",
    "    thermal_forcing_plus = thermal_forcing_all * big_constant_linear \n",
    "    \n",
    "    target_melt_Gt_yr = target_melt_all \n",
    "    \n",
    "    nisf_list = nisf_list_orig\n",
    "    \n",
    "    # LINEAR REGRESSION\n",
    "    # prepare data\n",
    "    thermal_forcing_term_stacked = thermal_forcing_plus.sel(Nisf=nisf_list).stack(reg_coord=('Nisf', 'time'))  \n",
    "    target_melt_stacked = target_melt_Gt_yr.sel(Nisf=nisf_list).stack(reg_coord=('Nisf', 'time'))  \n",
    "    \n",
    "    # choose domain and param\n",
    "    metrics_all = None\n",
    "\n",
    "    for dom in thermal_forcing_term_stacked.profile_domain:\n",
    "        metrics_list = [ ]\n",
    "\n",
    "        for mparam in thermal_forcing_term_stacked.param.values:\n",
    "\n",
    "            if bottom:\n",
    "                mparam0 = mparam[:-7:]\n",
    "            else:\n",
    "                mparam0 = mparam\n",
    "\n",
    "            if mparam0 != 'linear_local':\n",
    "                thermal_forcing_to_reg_stacked = thermal_forcing_term_stacked.sel(profile_domain=dom,param=mparam) * big_constant_quad\n",
    "            else:\n",
    "                thermal_forcing_to_reg_stacked = thermal_forcing_term_stacked.sel(profile_domain=dom,param=mparam)\n",
    "\n",
    "            if mparam0 != 'linear_local':\n",
    "                thermal_forcing_to_plot = thermal_forcing_plus.sel(Nisf=nisf_list).sel(profile_domain=dom,param=mparam) * big_constant_quad\n",
    "            else:\n",
    "                thermal_forcing_to_plot = thermal_forcing_plus.sel(Nisf=nisf_list).sel(profile_domain=dom,param=mparam)\n",
    "\n",
    "            # conduct the regression and compute the metrics\n",
    "            res, resid, rnk, s = linalg.lstsq(thermal_forcing_to_reg_stacked.values[:, np.newaxis], \n",
    "                                              target_melt_stacked.values)\n",
    "\n",
    "\n",
    "\n",
    "            metrics_da = xr.DataArray(data=res.squeeze()).to_dataset(name='slope')\n",
    "\n",
    "            metrics_list.append(metrics_da)\n",
    "\n",
    "        metrics_dom = xr.concat(metrics_list, dim='param')\n",
    "        metrics_dom = metrics_dom.assign_coords({'param': thermal_forcing_term_stacked.param})\n",
    "\n",
    "        metrics_all = meltf.merge_over_dim(metrics_dom, metrics_all, 'profile_domain', dom.values)\n",
    "\n",
    "    metrics_tt = meltf.merge_over_dim(metrics_all, metrics_tt, 'tblock', tt)\n",
    "\n",
    "metrics_tt.to_netcdf(outputpath_simple_all + 'gammas_simple_CV_time.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e579c93d-fdd2-4164-b760-b2f218447cf6",
   "metadata": {},
   "source": [
    "CROSS-VALIDATION ACROSS ICE SHELVES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca46404-7621-4145-b8f6-0c53d537fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_isf = None\n",
    "for isf_out in tqdm(nisf_list_orig):    \n",
    "\n",
    "    # PREPARE DATA\n",
    "\n",
    "    file_isf_list = []\n",
    "    thermal_forcing_term_list = []\n",
    "    target_melt_list = []\n",
    "    isf_area_lit = []\n",
    "    GL_flux_list = []\n",
    "    weights_list = []\n",
    "\n",
    "    for n,nemo_run in enumerate(run_list):\n",
    "\n",
    "        # File mask\n",
    "        inputpath_mask = home_path+'DATA/BASAL_MELT_PARAM/interim/ANTARCTICA_IS_MASKS/nemo_5km_'+nemo_run+'/'\n",
    "        file_isf_orig = xr.open_dataset(inputpath_mask+'nemo_5km_isf_masks_and_info_and_distance_new_oneFRIS.nc')\n",
    "        nonnan_Nisf = file_isf_orig['Nisf'].where(np.isfinite(file_isf_orig['front_bot_depth_max']), drop=True).astype(int)\n",
    "        file_isf_nonnan = file_isf_orig.sel(Nisf=nonnan_Nisf)\n",
    "        large_isf = file_isf_nonnan['Nisf'].where(file_isf_nonnan['isf_area_here'] >= 2500, drop=True)\n",
    "        isf_of_int = large_isf.drop_sel(Nisf=isf_out)\n",
    "        file_isf = file_isf_nonnan.sel(Nisf=isf_of_int)\n",
    "        file_isf_list.append(file_isf)\n",
    "\n",
    "        # File for param\n",
    "        outputpath_simple = home_path+'DATA/BASAL_MELT_PARAM/interim/SIMPLE/nemo_5km_'+nemo_run+'/'\n",
    "        if bottom:\n",
    "            thermal_forcing_2D = xr.open_dataset(outputpath_simple+'thermal_forcing_2D_bottom_for_tuning_corrected_oneFRIS.nc')\n",
    "            thermal_forcing_file = xr.open_dataset(outputpath_simple+'thermal_forcing_term_bottom_for_linreg_corrected_oneFRIS.nc')\n",
    "        else:\n",
    "            thermal_forcing_2D = xr.open_dataset(outputpath_simple+'thermal_forcing_2D_for_tuning_corrected_oneFRIS.nc')\n",
    "            thermal_forcing_file = xr.open_dataset(outputpath_simple+'thermal_forcing_term_for_linreg_corrected_oneFRIS.nc')\n",
    "        thermal_forcing_term = thermal_forcing_file['thermal_forcing_term'].sel(Nisf=isf_of_int)\n",
    "        thermal_forcing_term = thermal_forcing_term.assign_coords({'time': np.arange(1,len(thermal_forcing_term.time)+1)+n*50})\n",
    "        thermal_forcing_term_list.append(thermal_forcing_term)\n",
    "\n",
    "        # File for target\n",
    "        outputpath_melt = home_path+'DATA/BASAL_MELT_PARAM/processed/MELT_RATE/nemo_5km_'+nemo_run+'/'\n",
    "        NEMO_melt_rates_1D = xr.open_dataset(outputpath_melt+'melt_rates_1D_NEMO_oneFRIS.nc')\n",
    "        target_melt_Gt_yr = NEMO_melt_rates_1D['melt_Gt_per_y_tot'].sel(Nisf=isf_of_int).sel(time=thermal_forcing_file.time)\n",
    "        target_melt_Gt_yr = target_melt_Gt_yr.assign_coords({'time': np.arange(1,len(target_melt_Gt_yr.time)+1)+n*50})\n",
    "        target_melt_list.append(target_melt_Gt_yr)\n",
    "\n",
    "        isf_mask = file_isf['ISF_mask'].where(file_isf['ISF_mask'] == isf_of_int).sum('Nisf')\n",
    "        isf_mask = isf_mask.where(isf_mask)\n",
    "\n",
    "    file_isf_all = xr.concat(file_isf_list, dim='nemo_run')\n",
    "    #file_isf_all = file_isf_all.assign_coords({'nemo_run': run_list})\n",
    "\n",
    "    thermal_forcing_all = xr.concat(thermal_forcing_term_list, dim='time')\n",
    "    #thermal_forcing_all = thermal_forcing_all.assign_coords({'nemo_run': run_list})\n",
    "\n",
    "    target_melt_all = xr.concat(target_melt_list, dim='time')\n",
    "    #target_melt_all = target_melt_all.assign_coords({'nemo_run': run_list})\n",
    "    \n",
    "    # CONSTANTS AND TARGETS\n",
    "    \n",
    "    xx = file_isf_all.x\n",
    "    yy = file_isf_all.y\n",
    "    dx = (xx[2] - xx[1]).values\n",
    "    dy = (yy[2] - yy[1]).values\n",
    "    grid_cell_area = abs(dx*dy)\n",
    "    \n",
    "    # We need the big constant\n",
    "    big_constant_linear = rho_i * 10**-12 * yearinsec * grid_cell_area * melt_factor\n",
    "    big_constant_quad = (c_po / L_i) * beta_coeff_lazero * (g/(2*f_coriolis))\n",
    "    \n",
    "    # the thermal forcing term just missing the gamma (big_constant_quad is added later)\n",
    "    thermal_forcing_plus = thermal_forcing_all * big_constant_linear \n",
    "    \n",
    "    target_melt_Gt_yr = target_melt_all \n",
    "    \n",
    "    nisf_list = isf_of_int\n",
    "    \n",
    "    # LINEAR REGRESSION\n",
    "    # prepare data\n",
    "    thermal_forcing_term_stacked = thermal_forcing_plus.sel(Nisf=nisf_list).stack(reg_coord=('Nisf', 'time'))  \n",
    "    target_melt_stacked = target_melt_Gt_yr.sel(Nisf=nisf_list).stack(reg_coord=('Nisf', 'time'))  \n",
    "    \n",
    "    # choose domain and param\n",
    "    metrics_all = None\n",
    "\n",
    "    for dom in thermal_forcing_term_stacked.profile_domain:\n",
    "        metrics_list = [ ]\n",
    "\n",
    "        for mparam in thermal_forcing_term_stacked.param.values:\n",
    "\n",
    "            if bottom:\n",
    "                mparam0 = mparam[:-7:]\n",
    "            else:\n",
    "                mparam0 = mparam\n",
    "\n",
    "            if mparam0 != 'linear_local':\n",
    "                thermal_forcing_to_reg_stacked = thermal_forcing_term_stacked.sel(profile_domain=dom,param=mparam) * big_constant_quad\n",
    "            else:\n",
    "                thermal_forcing_to_reg_stacked = thermal_forcing_term_stacked.sel(profile_domain=dom,param=mparam)\n",
    "\n",
    "            if mparam0 != 'linear_local':\n",
    "                thermal_forcing_to_plot = thermal_forcing_plus.sel(Nisf=nisf_list).sel(profile_domain=dom,param=mparam) * big_constant_quad\n",
    "            else:\n",
    "                thermal_forcing_to_plot = thermal_forcing_plus.sel(Nisf=nisf_list).sel(profile_domain=dom,param=mparam)\n",
    "\n",
    "            # conduct the regression and compute the metrics\n",
    "            res, resid, rnk, s = linalg.lstsq(thermal_forcing_to_reg_stacked.values[:, np.newaxis], \n",
    "                                              target_melt_stacked.values)\n",
    "\n",
    "\n",
    "\n",
    "            metrics_da = xr.DataArray(data=res.squeeze()).to_dataset(name='slope')\n",
    "\n",
    "            metrics_list.append(metrics_da)\n",
    "\n",
    "        metrics_dom = xr.concat(metrics_list, dim='param')\n",
    "        metrics_dom = metrics_dom.assign_coords({'param': thermal_forcing_term_stacked.param})\n",
    "\n",
    "        metrics_all = meltf.merge_over_dim(metrics_dom, metrics_all, 'profile_domain', dom.values)\n",
    "\n",
    "    metrics_isf = meltf.merge_over_dim(metrics_all, metrics_isf, 'isf', isf_out)\n",
    "\n",
    "metrics_isf.to_netcdf(outputpath_simple_all + 'gammas_simple_CV_shelves.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1105e9c0",
   "metadata": {},
   "source": [
    "BOOTSTRAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d34208d-fccd-4805-b2a8-a03795df0dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath_chunk_info = '/bettik/burgardc/DATA/BASAL_MELT_PARAM/interim/T_S_PROF/'\n",
    "info_file = pd.read_csv(inputpath_chunk_info+'info_chunks.txt',header=None, index_col=0)\n",
    "\n",
    "# PREPARE DATA\n",
    "\n",
    "file_isf_list = []\n",
    "thermal_forcing_term_list = []\n",
    "target_melt_list = []\n",
    "time_list = []\n",
    "\n",
    "for n,nemo_run in enumerate(run_list):\n",
    "\n",
    "    # File mask\n",
    "    inputpath_mask = home_path+'DATA/BASAL_MELT_PARAM/interim/ANTARCTICA_IS_MASKS/nemo_5km_'+nemo_run+'/'\n",
    "    file_isf_orig = xr.open_dataset(inputpath_mask+'nemo_5km_isf_masks_and_info_and_distance_new_oneFRIS.nc')\n",
    "    nonnan_Nisf = file_isf_orig['Nisf'].where(np.isfinite(file_isf_orig['front_bot_depth_max']), drop=True).astype(int)\n",
    "    file_isf_nonnan = file_isf_orig.sel(Nisf=nonnan_Nisf)\n",
    "    large_isf = file_isf_nonnan['Nisf'].where(file_isf_nonnan['isf_area_here'] >= 2500, drop=True)\n",
    "    file_isf = file_isf_nonnan.sel(Nisf=large_isf)\n",
    "    file_isf_list.append(file_isf)\n",
    "\n",
    "    # File for param\n",
    "    outputpath_simple = home_path+'DATA/BASAL_MELT_PARAM/interim/SIMPLE/nemo_5km_'+nemo_run+'/'\n",
    "    if bottom:\n",
    "        thermal_forcing_2D = xr.open_dataset(outputpath_simple+'thermal_forcing_2D_bottom_for_tuning_corrected_oneFRIS.nc')\n",
    "        thermal_forcing_file = xr.open_dataset(outputpath_simple+'thermal_forcing_term_bottom_for_linreg_corrected_oneFRIS.nc')\n",
    "    else:\n",
    "        thermal_forcing_2D = xr.open_dataset(outputpath_simple+'thermal_forcing_2D_for_tuning_corrected_oneFRIS.nc')\n",
    "        thermal_forcing_file = xr.open_dataset(outputpath_simple+'thermal_forcing_term_for_linreg_corrected_oneFRIS.nc')\n",
    "\n",
    "    thermal_forcing_term_orig = thermal_forcing_file['thermal_forcing_term'].sel(Nisf=file_isf.Nisf)\n",
    "    thermal_forcing_term = thermal_forcing_term_orig.assign_coords({'time': np.arange(1,len(thermal_forcing_term_orig.time)+1)+n*50})\n",
    "    thermal_forcing_term_list.append(thermal_forcing_term)\n",
    "    time_list.append(thermal_forcing_term_orig.time.values)\n",
    "\n",
    "\n",
    "    # File for target\n",
    "    outputpath_melt = home_path+'DATA/BASAL_MELT_PARAM/processed/MELT_RATE/nemo_5km_'+nemo_run+'/'\n",
    "    NEMO_melt_rates_1D = xr.open_dataset(outputpath_melt+'melt_rates_1D_NEMO_oneFRIS.nc')\n",
    "    target_melt_Gt_yr = NEMO_melt_rates_1D['melt_Gt_per_y_tot'].sel(Nisf=file_isf.Nisf).sel(time=thermal_forcing_file.time)\n",
    "    target_melt_Gt_yr = target_melt_Gt_yr.assign_coords({'time': np.arange(1,len(target_melt_Gt_yr.time)+1)+n*50})\n",
    "    target_melt_list.append(target_melt_Gt_yr)\n",
    "\n",
    "    isf_mask = file_isf['ISF_mask'].where(file_isf['ISF_mask'] == file_isf.Nisf).sum('Nisf')\n",
    "    isf_mask = isf_mask.where(isf_mask)\n",
    "\n",
    "file_isf_all = xr.concat(file_isf_list, dim='nemo_run')\n",
    "#file_isf_all = file_isf_all.assign_coords({'nemo_run': run_list})\n",
    "\n",
    "thermal_forcing_all = xr.concat(thermal_forcing_term_list, dim='time')\n",
    "#thermal_forcing_all = thermal_forcing_all.assign_coords({'nemo_run': run_list})\n",
    "\n",
    "target_melt_all = xr.concat(target_melt_list, dim='time')\n",
    "#target_melt_all = target_melt_all.assign_coords({'nemo_run': run_list})\n",
    "\n",
    "yy_list = np.concatenate( time_list, axis=0 )\n",
    "\n",
    "n = 0\n",
    "n_list = ['OPM006']\n",
    "for tt in range(1,len(thermal_forcing_all.time)):\n",
    "    if (thermal_forcing_all.time[tt] - thermal_forcing_all.time[tt-1]) > 1:\n",
    "        n = n + 1\n",
    "    n_list.append(run_list[n])\n",
    "\n",
    "res_da_time = xr.DataArray(data=yy_list, dims= 'time', coords={'time': thermal_forcing_all.time}).to_dataset(name='years')\n",
    "res_da_runs = xr.DataArray(data=n_list, dims= 'time', coords={'time': thermal_forcing_all.time}).to_dataset(name='run')\n",
    "idx_ds = xr.merge([res_da_time,res_da_runs])\n",
    "\n",
    "# CONSTANTS AND TARGETS\n",
    "\n",
    "xx = file_isf_all.x\n",
    "yy = file_isf_all.y\n",
    "dx = (xx[2] - xx[1]).values\n",
    "dy = (yy[2] - yy[1]).values\n",
    "grid_cell_area = abs(dx*dy)\n",
    "\n",
    "# We need the big constant\n",
    "big_constant_linear = rho_i * 10**-12 * yearinsec * grid_cell_area * melt_factor\n",
    "big_constant_quad = (c_po / L_i) * beta_coeff_lazero * (g/(2*f_coriolis))\n",
    "\n",
    "# the thermal forcing term just missing the gamma (big_constant_quad is added later)\n",
    "thermal_forcing_plus = thermal_forcing_all * big_constant_linear \n",
    "\n",
    "target_melt_Gt_yr = target_melt_all \n",
    "\n",
    "tblock_dim = range(1,14)\n",
    "isf_dim = [10,11,12,13,18,22,23,24,25,30,31,33,38,39,40,42,43,44,45,47,48,51,52,53,54,55,58,61,65,66,69,70,71,73,75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2959c936-9c6c-4ec9-8b78-5f01dc2d23dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 15 # every iteration runs through 1000 samples (I went through iteration 1 to 15)\n",
    "\n",
    "# LOOP OVER BOOTSTRAP\n",
    "metrics_uncertainty = None\n",
    "all_isf_samples = None\n",
    "all_time_samples = None\n",
    "\n",
    "for n in tqdm(range(1000)):    \n",
    "    \n",
    "    np.random.seed((iteration-1)*1000+n)\n",
    "\n",
    "    # define the random bootstrap samples\n",
    "    random_tblock_sample = np.sort(np.random.choice(tblock_dim, size=len(tblock_dim), replace=True))\n",
    "    info_t = info_file.loc[random_tblock_sample].to_numpy()\n",
    "\n",
    "    random_time_list = []\n",
    "    rrun_list = []\n",
    "    for tt in random_tblock_sample:\n",
    "        run, tstart, tend = info_file.loc[tt]\n",
    "        random_time_list.append(idx_ds['time'].where((idx_ds['run']==run) & (idx_ds['years']>=tstart) & (idx_ds['years']<=tend)).dropna(dim='time').values.astype(int))\n",
    "        rrun_list.append(run)\n",
    "    time_idx = np.concatenate(random_time_list,axis=0)\n",
    "\n",
    "    final_run_list = []\n",
    "    for rr in run_list:\n",
    "        if rr in rrun_list:\n",
    "            final_run_list.append(rr)\n",
    "\n",
    "    random_isf_sample = np.random.choice(isf_dim, size=len(isf_dim), replace=True)\n",
    "        \n",
    "    # prepare data\n",
    "    thermal_forcing_term_stacked = thermal_forcing_plus.sel(Nisf=random_isf_sample, time=time_idx).unstack().stack(reg_coord=('Nisf', 'time')).sel(profile_domain=[50,1000])\n",
    "    target_melt_stacked = target_melt_Gt_yr.sel(Nisf=random_isf_sample, time=time_idx).unstack().stack(reg_coord=('Nisf', 'time'))\n",
    "    \n",
    "    # choose domain and param\n",
    "\n",
    "    metrics_all_unc = None\n",
    "\n",
    "    for dom in thermal_forcing_term_stacked.profile_domain:\n",
    "\n",
    "        metrics_list = [ ]\n",
    "\n",
    "        for mparam in thermal_forcing_term_stacked.param:\n",
    "\n",
    "            if mparam != 'linear_local':\n",
    "                thermal_forcing_to_reg_stacked = thermal_forcing_term_stacked.sel(profile_domain=dom,param=mparam)*big_constant_quad\n",
    "            else:\n",
    "                thermal_forcing_to_reg_stacked = thermal_forcing_term_stacked.sel(profile_domain=dom,param=mparam)\n",
    "\n",
    "            # conduct the regression and compute the metrics\n",
    "            res, resid, rnk, s = linalg.lstsq(thermal_forcing_to_reg_stacked.values[:, np.newaxis], target_melt_stacked.values)\n",
    "\n",
    "            yhat = thermal_forcing_to_reg_stacked*res\n",
    "            y = target_melt_stacked\n",
    "\n",
    "            metrics_da = xr.DataArray(data=res.squeeze()).to_dataset(name='slope')\n",
    "\n",
    "            metrics_list.append(metrics_da)\n",
    "\n",
    "        metrics_dom = xr.concat(metrics_list, dim='param')\n",
    "\n",
    "        metrics_all_unc = meltf.merge_over_dim(metrics_dom, metrics_all_unc, 'profile_domain', dom.values)\n",
    "\n",
    "    metrics_uncertainty = meltf.merge_over_dim(metrics_all_unc, metrics_uncertainty, 'bootstrap', n)\n",
    "    \n",
    "    \n",
    "    metrics_isf_sample = xr.DataArray(data=random_isf_sample, dims=['isf_dim'], coords={'bootstrap': n}).to_dataset(name='random_isf')\n",
    "    if len(time_idx) < 130:\n",
    "        new_time = np.zeros(130) * np.nan\n",
    "        for tt in range(len(time_idx)):\n",
    "            new_time[tt] = time_idx[tt]\n",
    "    metrics_time_sample = xr.DataArray(data=new_time, dims=['time_dim'], coords={'bootstrap': n}).to_dataset(name='random_time')\n",
    "    \n",
    "    all_isf_samples = meltf.merge_over_dim(metrics_isf_sample, all_isf_samples, 'bootstrap', n)\n",
    "    all_time_samples = meltf.merge_over_dim(metrics_time_sample, all_time_samples, 'bootstrap', n)\n",
    "\n",
    "all_info_metrics = xr.merge([metrics_uncertainty, all_isf_samples, all_time_samples])\n",
    "all_info_metrics.to_netcdf(outputpath_simple_all+'clusterbootstrap1000_'+str(iteration).zfill(2)+'_timeANDisf.nc')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ice_shelf_mask",
   "language": "python",
   "name": "ice_shelf_mask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
